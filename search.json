[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EPPS6356",
    "section": "",
    "text": "Here is the assignment 1 of Winter Tree image:\n\n\n\nWinter Tree\n\n\nHere is the assignment 1 of critique on a chart:\n critique: This chart provides a comprehensive view of the relationship between Loan Loss Reserves and Non-Performing Loans (NPLs), along with their corresponding percentages (Loan Loss Reserves % and NPL Rate) over time. It uses both bar charts and line graphs to present four key metrics from 2017 to 2022, with dual Y-axes to show dollar amounts on the left and percentages on the right. While the color coding and use of dual axes are strengths, the chart is overly complex with overlapping bars and inconsistent time intervals, making it harder to interpret. Separating the data into two charts—one for absolute values (Loan Loss Reserves and NPLs) and one for percentage trends—would improve clarity."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#assignment-1",
    "href": "index.html#assignment-1",
    "title": "EPPS6356",
    "section": "",
    "text": "Here is the assignment 1 of Winter Tree image:\n\n\n\nWinter Tree\n\n\nHere is the assignment 1 of critique on a chart:\n critique: This chart provides a comprehensive view of the relationship between Loan Loss Reserves and Non-Performing Loans (NPLs), along with their corresponding percentages (Loan Loss Reserves % and NPL Rate) over time. It uses both bar charts and line graphs to present four key metrics from 2017 to 2022, with dual Y-axes to show dollar amounts on the left and percentages on the right. While the color coding and use of dual axes are strengths, the chart is overly complex with overlapping bars and inconsistent time intervals, making it harder to interpret. Separating the data into two charts—one for absolute values (Loan Loss Reserves and NPLs) and one for percentage trends—would improve clarity."
  },
  {
    "objectID": "index.html#assignment-2",
    "href": "index.html#assignment-2",
    "title": "EPPS6356",
    "section": "Assignment 2",
    "text": "Assignment 2\ndata &lt;- read_excel(“C:/R/coursework/EPPS6356/test1/_site/HPI_country_data.xlsx”)\ncolnames(data)\ngdp &lt;- data\\(`GDP per capita (\\))life_expectancy &lt;- data$Life Expectancy (years)population &lt;- data$Population (thousands)wellbeing &lt;- data$Ladder of life (Wellbeing) (0-10)` hpi &lt;- data$HPI\nx &lt;- gdp[1:50] y1 &lt;- wellbeing[1:50] y2 &lt;- life_expectancy[1:50]\npar(mfrow = c(3, 2))\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) plot.new() plot.window(range(x, na.rm = TRUE), range(c(y1, y2), na.rm = TRUE)) lines(x, y1, col=“blue”) lines(x, y2, col=“red”)\npoints(x, y1, pch=16, cex=2, col=“blue”)\npoints(x, y2, pch=21, bg=“white”, cex=2, col=“red”) par(col=“gray50”, fg=“gray50”, col.axis=“gray50”) axis(1, at=seq(min(x, na.rm = TRUE), max(x, na.rm = TRUE), length.out=5))\naxis(2, at=seq(min(y1, na.rm = TRUE), max(y1, na.rm = TRUE), length.out=5))\naxis(4, at=seq(min(y2, na.rm = TRUE), max(y2, na.rm = TRUE), length.out=5))\nbox(bty=“u”)\nmtext(“GDP per capita”, side=1, line=2, cex=0.8)\nmtext(“Wellbeing”, side=2, line=2, las=0, cex=0.8)\nmtext(“Life Expectancy”, side=4, line=2, las=0, cex=0.8)\ntext(gdp[5], wellbeing[5], data$Country[5], col=“blue”)\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=“black”, fg=“black”, col.axis=“black”)\ngdp_clean &lt;- gdp[!is.na(gdp)] par(mar=c(4.5, 4.1, 3.1, 0)) hist(gdp_clean, breaks=20, col=“gray80”, freq=FALSE)\nlines(density(gdp_clean, na.rm=TRUE), lwd=2) par(mar=c(5.1, 4.1, 4.1, 2.1))\npar(mar=c(2, 3.1, 2, 2.1)) barplot(gdp[1:50], names.arg = data$Country[1:50], las=2, col=gray(seq(0.1, 0.5, length=50)), main=“GDP of Countries”) par(mar=c(5.1, 4.1, 4.1, 2.1))\npar(mar=c(3, 4.1, 2, 0)) boxplot(gdp ~ population, data = data[1:50, ], col=“white”, main=“Boxplot of GDP by Population”) mtext(“Population”, side=1, line=2.5, cex=0.8)\nx &lt;- gdp[1:50] y &lt;- life_expectancy[1:50] z &lt;- wellbeing[1:50] par(mar=c(0, 0.5, 0, 0), lwd=0.5) persp(x, y, outer(x, y, function(x, y) x * y / max(x, na.rm=TRUE)), theta = 30, phi = 30, expand = 0.5) par(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5) population_data &lt;- population[1:50] names(population_data) &lt;- data$Country[1:50] pie(population_data, col = gray(seq(0.3, 1.0, length=50)), main=“Population Distribution of Countries”)\nHere is the assignment 2 of excerse image:"
  },
  {
    "objectID": "index.html#note",
    "href": "index.html#note",
    "title": "EPPS6356",
    "section": "Note",
    "text": "Note\nThe pitfalls of big data analytics mainly manifest in two areas:\nBlind Faith in Big Data: Over-reliance on big data can lead to the misconception that large datasets can replace traditional data analysis methods, overlooking fundamental issues such as measurement validity and data reliability. While big data can provide abundant information, if the accuracy and relevance of the data to the research objective are not considered, it can lead to incorrect conclusions. For example, simply relying on search data to predict flu trends might cause bias due to other seasonal factors (such as the popularity of sports events), which are unrelated to the actual flu outbreak.\nAlgorithm Dynamics: Over time, platforms like Google continuously adjust their algorithms, altering how data is generated. These changes in the search algorithm and user behavior can make models unstable, resulting in predictions that deviate from real-world conditions. This dynamic nature makes it difficult to ensure the accuracy of big data predictions.\nAnother common issue in big data modeling is overfitting and overparameterization:\nOverfitting: In analysis, if the model fits the noise in the training data too closely rather than capturing the true patterns in the data, its predictive performance will deteriorate. For instance, when analyzing flu data, removing search terms unrelated to the flu but statistically correlated may seem helpful, but it can cause the model to miss out on actual predictive factors, leading to poor predictions on new data.\nOverparameterization: Using too many variables to fit a small number of data points results in overly complex models, which are more likely to capture spurious correlations. In such cases, the model may not only fail to generalize to other datasets but also significantly reduce the stability and accuracy of its predictions.\nIn conclusion, while big data is powerful, if not used cautiously, it can easily fall into the traps of blind faith, overfitting, and overparameterization. Therefore, when using big data, it is crucial to combine it with traditional data analysis methods and maintain a reasonable level of model complexity."
  }
]